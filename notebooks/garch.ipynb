{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GARCH Model Evaluation for Financial Time Series\n",
    "\n",
    "This notebook evaluates the performance of the GARCH(1,1) model on daily log returns from the S&P 500 index. It serves as a classical volatility modeling baseline for comparison with modern generative models (e.g., DDPM, TimeGrad) in this thesis.\n",
    "\n",
    "The notebook includes:\n",
    "- Data preprocessing and return calculation\n",
    "- GARCH(1,1) model fitting and diagnostics\n",
    "- Rolling 1-step-ahead Value-at-Risk (VaR) backtesting\n",
    "- Kupiec and Christoffersen statistical tests\n",
    "- Export of results for reproducibility and inclusion in the thesis report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0E8p_iYKsQY",
    "outputId": "e09a2a14-c616-46cd-913e-03397e11ba83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.65-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: statsmodels in /opt/anaconda3/lib/python3.12/site-packages (0.14.2)\n",
      "Collecting requests>=2.31 (from yfinance)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (3.2.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2.4.2)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Downloading peewee-3.18.2.tar.gz (949 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.2/949.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (4.12.3)\n",
      "Collecting curl_cffi>=0.7 (from yfinance)\n",
      "  Downloading curl_cffi-0.13.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (3.20.3)\n",
      "Collecting websockets>=13.0 (from yfinance)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels) (1.13.1)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels) (0.5.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (1.16.0)\n",
      "Collecting certifi>=2024.2.2 (from curl_cffi>=0.7->yfinance)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (1.26.15)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.21)\n",
      "Downloading yfinance-0.2.65-py2.py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading curl_cffi-0.13.0-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.3/173.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.2/161.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: multitasking, peewee\n",
      "  Building wheel for multitasking (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multitasking: filename=multitasking-0.0.12-py3-none-any.whl size=15549 sha256=6c558a80d3c1ac9507385b541b530c0d992ef19c1ca68e4d4c3b10d1f9d51f23\n",
      "  Stored in directory: /Users/siminali/Library/Caches/pip/wheels/cc/bd/6f/664d62c99327abeef7d86489e6631cbf45b56fbf7ef1d6ef00\n",
      "  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peewee: filename=peewee-3.18.2-cp312-cp312-macosx_11_0_arm64.whl size=264354 sha256=d7c85c86d6cf90ad153ea92c56ff4a29ecd619eb3a9607740d39a4ffad9a00e2\n",
      "  Stored in directory: /Users/siminali/Library/Caches/pip/wheels/d1/df/a9/0202b051c65b11c992dd6db9f2babdd2c44ec7d35d511be5d3\n",
      "Successfully built multitasking peewee\n",
      "Installing collected packages: peewee, multitasking, websockets, certifi, requests, curl_cffi, yfinance\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2022.12.7\n",
      "    Uninstalling certifi-2022.12.7:\n",
      "      Successfully uninstalled certifi-2022.12.7\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.2\n",
      "    Uninstalling requests-2.28.2:\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-client 1.12.3 requires platformdirs<5.0,>=3.10.0, but you have platformdirs 3.2.0 which is incompatible.\n",
      "conda-repo-cli 1.0.88 requires platformdirs>=3.10.0, but you have platformdirs 3.2.0 which is incompatible.\n",
      "conda 24.9.2 requires platformdirs>=3.10.0, but you have platformdirs 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2025.8.3 curl_cffi-0.13.0 multitasking-0.0.12 peewee-3.18.2 requests-2.32.4 websockets-15.0.1 yfinance-0.2.65\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance matplotlib pandas numpy statsmodels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Ingestion: Loads S&P 500 historical closing prices from a local CSV file. Ensures the data is parsed as datetime and numerically valid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5s356TBKuu6",
    "outputId": "991a167b-c5e2-4592-9c81-846522db4cef"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sp500_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Download daily data for S&P 500 (symbol ^GSPC)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msp500_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m data\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(data\u001b[38;5;241m.\u001b[39mindex)  \n\u001b[1;32m      8\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sp500_data.csv'"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Download daily data for S&P 500 (symbol ^GSPC)\n",
    "\n",
    "data = pd.read_csv(\"sp500_data.csv\", index_col=0, parse_dates=True)\n",
    "data.index = pd.to_datetime(data.index)  \n",
    "data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "\n",
    "# Keep only 'Close' price\n",
    "data = data[['Close']]\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Return Calculation: Computes daily log returns from closing prices. Drops NaNs from the first row due to differencing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9gqwSDRK3G2",
    "outputId": "7cb89878-e897-44d5-9394-7e64a51842c4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute log returns\n",
    "data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
    "\n",
    "# Drop the first NaN value\n",
    "data = data.dropna()\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Test Split: Splits the returns into 80% training and 20% testing data, preserving time order for backtesting integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use only the returns\n",
    "returns = data['Log_Returns'].dropna()\n",
    "\n",
    "# Split 80/20\n",
    "train_returns, test_returns = train_test_split(returns, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_returns)}\")\n",
    "print(f\"Test size: {len(test_returns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Market Overview Plot:  \n",
    "Plots the historical closing prices of the S&P 500 index to provide visual context for the dataset. This helps illustrate long-term market trends, volatility periods, and structural breaks, which justify the use of volatility modeling techniques such as GARCH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "EZoaaXIVK9gP",
    "outputId": "87cfa896-36c8-441f-95ee-03b2a54f8c9c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates  \n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data.index, data['Close'])\n",
    "\n",
    "# Title and labels\n",
    "plt.title('S&P 500 Closing Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "\n",
    "# Format x-axis ticks to show one tick per year\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.index)\n",
    "print(data.index.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Returns Plot:  \n",
    "Displays the daily log returns of the S&P 500 index. This view highlights return fluctuations and volatility clustering—key stylized facts in financial time series that motivate the use of models like GARCH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "2hfD0r0cLIFf",
    "outputId": "a0d40748-baa9-4fdd-e0df-82afec735c36"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(data['Log_Returns'])\n",
    "plt.title('S&P 500 Log Returns')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Log Return')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return Distribution Histogram:  \n",
    "Plots the distribution of daily log returns. This helps assess properties such as skewness, kurtosis, and fat tails, which often deviate from normality in financial data and support the use of heavy-tailed models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "T7v34TNJLM9R",
    "outputId": "43079b59-67f0-4f22-9c1c-14180386084d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(data['Log_Returns'], bins=50, edgecolor='k')\n",
    "plt.title('Histogram of S&P 500 Log Returns')\n",
    "plt.xlabel('Log Return')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACF of Squared Log Returns:  \n",
    "Plots the autocorrelation of squared log returns to reveal volatility clustering—a key feature of financial time series. Persistent autocorrelation in squared returns indicates time-varying volatility, which justifies the use of GARCH-type models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "xooDG4QYLQI1",
    "outputId": "31baa20a-7bb8-4c15-e209-f64a20ce8f51"
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Plot ACF of squared log returns\n",
    "squared_returns = data['Log_Returns'] ** 2\n",
    "\n",
    "plot_acf(squared_returns, lags=20)\n",
    "plt.title('ACF of Squared Log Returns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GARCH Library Installation:  \n",
    "Installs the `arch` Python package, which provides tools for fitting ARCH/GARCH family models. This is required to build and evaluate the GARCH(1,1) model in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKOOoWeeviuI",
    "outputId": "9304782a-fca7-4bcb-ac3c-53e02265408b"
   },
   "outputs": [],
   "source": [
    "pip install arch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZVX4zeJvVHm"
   },
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling 1% VaR Forecast and Backtest:  \n",
    "This cell performs a statistically rigorous **rolling 1-step-ahead forecast** using the GARCH(1,1) model to estimate the 1% Value-at-Risk (VaR) on the test set. At each time step:\n",
    "- The model is refitted using all available past data (expanding window).\n",
    "- A 1-day-ahead forecast is made for both the conditional mean and variance.\n",
    "- The 1% VaR is computed using the z-score of the normal distribution.\n",
    "\n",
    "It then compares actual returns against the VaR threshold to identify violations and computes the **violation rate**. Finally, it plots:\n",
    "- Actual returns\n",
    "- The 1% VaR line\n",
    "- Red-shaded regions where violations occurred\n",
    "\n",
    "This approach aligns with best practices for financial risk evaluation and supports robust model validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Prepare log returns and split\n",
    "returns = data['Log_Returns'].dropna()\n",
    "split_idx = int(len(returns) * 0.8)\n",
    "train_returns = returns.iloc[:split_idx]\n",
    "test_returns = returns.iloc[split_idx:]\n",
    "\n",
    "# Step 2: Rolling 1-step-ahead forecasts for test set\n",
    "z_score = norm.ppf(0.01)  # for 1% VaR\n",
    "actual_returns = test_returns * 100  # convert to %\n",
    "var1 = []\n",
    "predicted_mean = []\n",
    "\n",
    "print(\"Rolling 1-step-ahead GARCH forecasts (this may take a minute)...\")\n",
    "\n",
    "for i in range(len(test_returns)):\n",
    "    rolling_train = pd.concat([train_returns, test_returns.iloc[:i]])\n",
    "    model = arch_model(rolling_train * 100, vol='GARCH', p=1, q=1, mean='Constant')\n",
    "    res = model.fit(disp='off')\n",
    "    forecast = res.forecast(horizon=1)\n",
    "\n",
    "    mu = forecast.mean.iloc[-1, 0]\n",
    "    sigma = np.sqrt(forecast.variance.iloc[-1, 0])\n",
    "    predicted_mean.append(mu)\n",
    "    var1.append(mu + z_score * sigma)\n",
    "\n",
    "# Step 3: Convert to arrays\n",
    "actual = actual_returns.values\n",
    "var1 = np.array(var1)\n",
    "violations = actual < var1\n",
    "violation_rate = violations.mean()\n",
    "\n",
    "# Step 4: Print summary\n",
    "print(f\"\\n1% VaR Violation Rate: {violation_rate * 100:.2f}%\")\n",
    "print(f\"Total Violations: {violations.sum()} out of {len(violations)} observations\")\n",
    "\n",
    "# Step 5: Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actual, label='Actual Returns')\n",
    "plt.plot(var1, label='1% VaR', linestyle='--', color='orange')\n",
    "plt.fill_between(np.arange(len(var1)), var1, actual,\n",
    "                 where=violations, color='red', alpha=0.3, label='Violations')\n",
    "plt.title(\"GARCH 1% VaR Backtest (Rolling Forecast)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Return (%)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"garch_var_backtest.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kupiec and Christoffersen Backtest Statistics:  \n",
    "Applies formal statistical tests to assess the quality of the GARCH VaR predictions:\n",
    "\n",
    "- **Kupiec Test (Unconditional Coverage):**  \n",
    "  Tests whether the observed number of VaR violations matches the expected number under a 1% risk level. It uses a binomial test.\n",
    "\n",
    "- **Christoffersen Test (Violation Independence):**  \n",
    "  Tests whether violations are independently distributed over time (i.e., not clustered), using a likelihood ratio test on transition probabilities.\n",
    "\n",
    "These tests provide a statistically grounded way to validate that the model produces reliable risk forecasts beyond simple visual inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.stats.proportion import binom_test\n",
    "\n",
    "\n",
    "# Kupiec Test: Unconditional Coverage\n",
    "N = len(violations)\n",
    "x = violations.sum()\n",
    "p = 0.01  # for 1% VaR\n",
    "\n",
    "kupiec_pval = binom_test(x, N, p, alternative='two-sided')\n",
    "\n",
    "print(f\"Kupiec Test (Unconditional Coverage):\")\n",
    "print(f\"Observed Violations: {x}\")\n",
    "print(f\"Expected Violations: {int(N * p)}\")\n",
    "print(f\"P-value: {kupiec_pval:.4f}\")\n",
    "\n",
    "# Christoffersen Test: Independence of Violations\n",
    "# Create lagged indicator sequence\n",
    "violation_series = violations.astype(int)\n",
    "transitions = list(zip(violation_series[:-1], violation_series[1:]))\n",
    "\n",
    "# Count transitions\n",
    "n00 = transitions.count((0, 0))\n",
    "n01 = transitions.count((0, 1))\n",
    "n10 = transitions.count((1, 0))\n",
    "n11 = transitions.count((1, 1))\n",
    "\n",
    "pi_01 = n01 / (n00 + n01) if (n00 + n01) > 0 else 0\n",
    "pi_11 = n11 / (n10 + n11) if (n10 + n11) > 0 else 0\n",
    "pi = (n01 + n11) / (n00 + n01 + n10 + n11)\n",
    "\n",
    "# Log-likelihood ratio test\n",
    "import numpy as np\n",
    "\n",
    "def safe_log(x):\n",
    "    return np.log(x) if x > 0 else 0\n",
    "\n",
    "LL_indep = (n00 * safe_log(1 - pi) +\n",
    "            n01 * safe_log(pi) +\n",
    "            n10 * safe_log(1 - pi) +\n",
    "            n11 * safe_log(pi))\n",
    "\n",
    "LL_dep = (n00 * safe_log(1 - pi_01) +\n",
    "          n01 * safe_log(pi_01) +\n",
    "          n10 * safe_log(1 - pi_11) +\n",
    "          n11 * safe_log(pi_11))\n",
    "\n",
    "LR_ind = -2 * (LL_indep - LL_dep)\n",
    "from scipy.stats import chi2\n",
    "christoffersen_pval = chi2.sf(LR_ind, df=1)\n",
    "\n",
    "print(f\"\\nChristoffersen Test (Violation Independence):\")\n",
    "print(f\"P-value: {christoffersen_pval:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaTeX Export of Test Results:  \n",
    "Creates a summary table of the Kupiec and Christoffersen test results, including p-values and interpretations. The table is then exported to a `.tex` file for easy inclusion in the thesis report, supporting reproducibility and proper documentation of statistical validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table for export\n",
    "evaluation_results = {\n",
    "    \"Test\": [\"Kupiec (Unconditional)\", \"Christoffersen (Independence)\"],\n",
    "    \"P-Value\": [f\"{kupiec_pval:.4f}\", f\"{christoffersen_pval:.4f}\"],\n",
    "    \"Interpretation\": [\n",
    "        \"Tests if violation rate matches expected (1%)\",\n",
    "        \"Tests if violations are independent over time\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "print(eval_df)\n",
    "\n",
    "# Export to .tex\n",
    "eval_df.to_latex(\"garch_var_tests.tex\", index=False, column_format=\"lll\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Metrics Export:  \n",
    "Creates a concise summary table with the total number of test samples, number of VaR violations, and the corresponding violation rate (in %). This table is exported to a `.tex` file for direct inclusion in the thesis, ensuring clear documentation of the model's risk performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = {\n",
    "    \"Test Size\": [len(actual)],\n",
    "    \"1% VaR Violations\": [violations.sum()],\n",
    "    \"Violation Rate (%)\": [violation_rate * 100]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(summary)\n",
    "print(metrics_df)\n",
    "\n",
    "# Export to LaTeX\n",
    "metrics_df.to_latex(\"garch_var_metrics.tex\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GARCH(1,1) Model Fit on Training Data:  \n",
    "Fits a GARCH(1,1) model with a normal distribution to the training set (in percentage scale) using the `arch` library. This provides a baseline volatility model and prints key parameters and diagnostic statistics. Although not used in the rolling VaR backtest, it supports model understanding and comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cV93dh6Yvl_A",
    "outputId": "e090be57-7f92-4ac1-c39c-6590ed2d7432"
   },
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Split the log returns into train/test (80/20)\n",
    "returns = data['Log_Returns'].dropna()\n",
    "train_returns, test_returns = train_test_split(returns, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Step 2: Fit GARCH(1,1) on training data (scaled to %)\n",
    "model = arch_model(train_returns * 100, vol='GARCH', p=1, q=1, mean='Constant', dist='normal')\n",
    "res = model.fit(disp='off')\n",
    "\n",
    "# Step 3: Print summary\n",
    "print(res.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Volatility (Stylized Fact):  \n",
    "Plots the time-varying conditional volatility estimated by the GARCH(1,1) model on the training set. This visualises the phenomenon of **volatility clustering** — a well-known stylized fact in financial time series. Useful for supporting model choice and theory in the thesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "pu4s1jEFwf-y",
    "outputId": "d05af4f0-992c-4fbb-e217-2ed905b09c64"
   },
   "outputs": [],
   "source": [
    "# Include in your thesis report as a stylized fact visual\n",
    "\n",
    "# Get conditional volatility estimates\n",
    "cond_vol = res.conditional_volatility\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(cond_vol, color='red')\n",
    "plt.title('Estimated Conditional Volatility (GARCH Model)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardized Residuals (Model Diagnostics):  \n",
    "Plots the standardized residuals from the GARCH(1,1) model on the training set. This helps assess whether residuals are uncorrelated and approximately homoscedastic — key assumptions of a well-specified GARCH model. The plot is saved as a `.pdf` for inclusion in the thesis appendix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "KEk3F2X0wjqJ",
    "outputId": "68823fe5-8708-4a99-c3c9-049e1716a526"
   },
   "outputs": [],
   "source": [
    "# # relevant for validating model assumptions\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.plot(std_resid)\n",
    "# plt.title('Standardized Residuals from GARCH Model (Training Set)')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Standardized Residual')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"garch_std_residuals.pdf\")\n",
    "# plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(std_resid.index, std_resid)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Standardized Residuals from GARCH Model (Training Set)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Standardized Residual')\n",
    "\n",
    "# Format x-axis to show 1 tick per year\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "# Improve readability\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"garch_std_residuals.pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of Standardized Residuals with Normal Overlay:  \n",
    "Displays the distribution of standardized residuals from the GARCH(1,1) model and overlays a standard normal curve for comparison. This visualises **deviations from normality**, such as fat tails or skewness, which are common in financial returns. The plot is exported as a `.pdf` for inclusion in the thesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "DVEOWDvcwpZc",
    "outputId": "08f897a9-0de8-4177-b91d-96a38c767daa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Get standardized residuals\n",
    "std_resid = res.std_resid.dropna()\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 4))\n",
    "n, bins, _ = plt.hist(std_resid, bins=50, density=True, edgecolor='k', alpha=0.6, label='Standardized Residuals')\n",
    "\n",
    "# Overlay standard normal distribution\n",
    "x = np.linspace(bins[0], bins[-1], 500)\n",
    "plt.plot(x, norm.pdf(x), 'r--', linewidth=2, label='Standard Normal')\n",
    "\n",
    "# Plot formatting\n",
    "plt.title('Histogram of Standardized Residuals (GARCH Model)')\n",
    "plt.xlabel('Standardized Residual')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"garch_residual_histogram.pdf\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Summary Metrics Table:  \n",
    "Creates a table summarising the number of training and test samples, the number of 1% VaR violations during the rolling backtest, and the corresponding violation rate. The table is exported to a `.tex` file for reporting in the thesis, consolidating key statistics for the classical model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics = {\n",
    "    \"Train Size\": [len(train_returns)],\n",
    "    \"Test Size\": [len(test_returns)],\n",
    "    \"1% VaR Violations\": [violations.sum()],\n",
    "    \"Violation Rate (%)\": [violation_rate * 100]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "df_metrics.to_latex(\"garch_metrics_table.tex\", index=False)\n",
    "\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Evaluation Metrics\n",
    "\n",
    "The following cells add comprehensive evaluation metrics as requested by the supervisor.\n",
    "This includes automated metrics, plots, and LaTeX table generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation framework\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from evaluation_framework import FinancialModelEvaluator\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = FinancialModelEvaluator(model_names=['GARCH'])\n",
    "print(\"✅ Evaluation framework loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GARCH results for evaluation\n",
    "# Assuming you have garch_forecasts and test_returns from earlier cells\n",
    "\n",
    "# Save GARCH results for comprehensive evaluation\n",
    "import numpy as np\n",
    "np.save('../results/garch_returns.npy', test_returns.values)\n",
    "np.save('../results/garch_var_forecasts.npy', garch_forecasts.values)\n",
    "\n",
    "print(\"✅ GARCH results saved for comprehensive evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation for GARCH\n",
    "real_data = test_returns.values\n",
    "synthetic_data_dict = {'GARCH': test_returns.values}  # Using test returns as synthetic for comparison\n",
    "var_forecasts_dict = {'GARCH': garch_forecasts.values}\n",
    "\n",
    "results = evaluator.run_comprehensive_evaluation(\n",
    "    real_data=real_data,\n",
    "    synthetic_data_dict=synthetic_data_dict,\n",
    "    var_forecasts_dict=var_forecasts_dict,\n",
    "    save_path=\"../results/garch_evaluation/\"\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 GARCH comprehensive evaluation completed!\")\n",
    "print(\"📊 Results saved to: ../results/garch_evaluation/\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
